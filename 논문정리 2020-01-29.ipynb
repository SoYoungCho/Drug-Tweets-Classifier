{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 01.29 -  Recently updated.\n",
    "\n",
    "> 01.21 - **Add preprocess funcs**  \n",
    "> 01.29 - **Add Models** - on working, -making `class Models`\n",
    "\n",
    "**Checklist**\n",
    "- [x] 1.preprocess_func\n",
    "- [ ] 2.models\n",
    "- [ ] 3.confusion_matrix  \n",
    "- [ ] 4.collect data   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Drug-Tweet-Classifier\n",
    "---\n",
    "# 순서\n",
    "**\n",
    " 1. 모듈 불러오기 및 파일 읽기\n",
    " 2. 전처리 \n",
    "   - 1) 특수문자, 숫자, URL 제거\n",
    "   - 2) 형태소 추출\n",
    "   - 3) 자소 분리  \n",
    " 3. 단어 임베딩(with Tf-Idf)\n",
    " 4. 모델 투입\n",
    " 5. 성능비교(Confusion Matrix)  \n",
    " (6). ~ 부족한 경우 단어 추가\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Module & Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기본 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#전처리부분\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#모델부분\n",
    "from sklearn import naive_bayes, svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#검증부분 - Confusion_Matrix / Preciion and Reall\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score , recall_score , f1_score\n",
    "\n",
    "#검증부분 - Cross Validation\n",
    "from sklearn.model_selection import cross_val_score #점수 나타내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일을 읽고 필요한 정보 데이터만 가져오기\n",
    "def read_file(file):\n",
    "    data = pd.read_excel(file)\n",
    "    text = data[\"text\"]\n",
    "    label = data['label']\n",
    "    new_data = pd.DataFrame({\"Text\" : text,\"Label\" : label})\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= read_file('real6.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106448"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_all_sentence(text_data): # type(text_data) => DataFrame\n",
    "    result = []\n",
    "    count = 0\n",
    "    for idx,text in enumerate(list(text_data)): # 수정 예정\n",
    "        temp = text.split(sep = \" \")\n",
    "        result.append(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data에 있는 text 데이터에 대해 url 전부 제거\n",
    "def remove_url(text_data):\n",
    "    result = []\n",
    "    for line in text_data: \n",
    "        new_line = []\n",
    "        for text in line: # 단어별로 나눠진 텍스트들이 http, pic을 담고있다면 문장에서 제외\n",
    "            if (\"http\" not in text) and (\"pic\" not in text):\n",
    "                new_line.append(text) \n",
    "        result.append(new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 기존 코드 - 특문 제거\n",
    "\n",
    "---\n",
    "```python\n",
    "#특수문자를 모두 제거하는 함수\n",
    "\n",
    "def change(otl):\n",
    "    result = []\n",
    "    for line in otl:\n",
    "        result_text = \"\"\n",
    "        for word in line:\n",
    "            if word.isalpha() : # 단어에 특문이 포함되지 않은 경우\n",
    "                result_text += word\n",
    "            else:\n",
    "                for t in word: # 단어에 특문, 숫자가 포함된 경우 문자별로 파악\n",
    "                    if t.isalpha():\n",
    "                        result_text += t\n",
    "                    else:\n",
    "                        result_text += \" \" # 사라질 문자는 'space' 로 대체 => 5) 조정 필요\n",
    "            result_text += \" \"\n",
    "        result.append(result_text)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 기존 코드 - 공백을 없애는 함수\n",
    "\n",
    "\n",
    "```python\n",
    "### 기존함수 change의 공백때문에 필요했던 함수.\n",
    "## 지금은 필요없음\n",
    "\n",
    "def trim_spacing(after_remove_text):\n",
    "    new_text = []\n",
    "    for line in after_remove_text: # 한 줄씩 읽기.\n",
    "        tmp = []\n",
    "        for word in line.split(): # split(delimeter = whitespace) > default\n",
    "            if word != \" \":\n",
    "                tmp.append(word)\n",
    "        new_text.append(\" \".join(tmp))\n",
    "    print(new_text)\n",
    "```\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 remove_special로 대체하면 한방에 해결\n",
    "\n",
    "def remove_special(after_remove_url): # 특수문자제거\n",
    "    result = []\n",
    "    for i, line in enumerate(after_remove_url):\n",
    "        tmp = ' '.join([word if word.isalpha() else (''.join([w for w in word if w.isalpha()])) for word in line])\n",
    "        result.append(tmp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = split_all_sentence(data['Text']) # 텍스트데이터만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "rurl_text = remove_url(text_data) # url 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_text = remove_special(rurl_text) # 특문제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사람들이 잘 모르는데 진짜 유명해졌으면 하는 일본 편의점 음식 일본 미니스톱 아이스 果実いちご氷 세상사람들 이것 좀 먹어보세요ㅠ유유ㅠㅠㅠ 위에는 달달한 바닐라 아이스고 밑에는 얼린 딸기에 연유가 뿌려져',\n",
       " '아이스의오락시간 아이스 의 오락시간 과연 누가 아이스의 유연성 왕일지',\n",
       " '오 오랫만에 아이스 돌체라떼 음 맛있겠다  Starbucks in 부산광역시 Busan',\n",
       " '맛집탐험대 나를 놀라게한 커피 TERAROSA TERAROSA 테라로사 날씨 냉커피 덥다 맛 맛집 사장님 서정점 아메리카노 아이스 아이스커피 양평 좋다 카페 커피 커피향 커피향기 향기 홍보',\n",
       " '로렉스 아이스 다이아고씨쥬얼리명품시계금화이트골드화이트골드']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_text[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2-1) Preprocessing - 형태소 추출\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = rs_text[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사람들이 잘 모르는데 진짜 유명해졌으면 하는 일본 편의점 음식 일본 미니스톱 아이스 果実いちご氷 세상사람들 이것 좀 먹어보세요ㅠ유유ㅠㅠㅠ 위에는 달달한 바닐라 아이스고 밑에는 얼린 딸기에 연유가 뿌려져',\n",
       " '아이스의오락시간 아이스 의 오락시간 과연 누가 아이스의 유연성 왕일지',\n",
       " '오 오랫만에 아이스 돌체라떼 음 맛있겠다  Starbucks in 부산광역시 Busan']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNG,*,T,사람,*,*,*,*', 'XSN,*,T,들,*,*,*,*', 'JKS,*,F,이,*,*,*,*', 'MAG,*,T,잘,*,*,*,*', 'VV,*,F,모르,*,*,*,*', 'EC,*,F,는데,*,*,*,*', 'MAG,문장부사|양상부사,F,진짜,*,*,*,*', 'XR,*,T,유명,*,*,*,*', 'XSA+EC+VX+EP+EC,*,T,해졌으면,Inflect,XSA,EC,하/XSA/*+아/EC/*+지/VX/*+었/EP/*+으면/EC/*', 'VV,*,F,하,*,*,*,*', 'ETM,*,T,는,*,*,*,*', 'NNP,지명,T,일본,*,*,*,*', 'NNG,*,T,편의점,Compound,*,*,편의/NNG/정적사태+점/NNG/*', 'NNG,*,T,음식,*,*,*,*', 'NNP,지명,T,일본,*,*,*,*', 'NNP,*,T,미니스톱,Compound,*,*,미니/NNG/*+스톱/NNG/*', 'NNP,인명,F,아이스,*,*,*,*', 'NNG,*,F,과,*,*,*,*', 'SH,*,*,*,*,*,*,*', 'SL,*,*,*,*,*,*,*', 'SH,*,*,*,*,*,*,*', 'NNG,*,T,세상,*,*,*,*', 'NNG,*,T,사람,*,*,*,*', 'XSN,*,T,들,*,*,*,*', 'NP,*,T,이것,Inflect,NP,NP,이거/NP/*', 'MAG,성분부사|정도부사,T,좀,*,*,*,*', 'VV,*,T,먹,*,*,*,*', 'EC,*,F,어,*,*,*,*', 'VX,*,F,보,*,*,*,*', 'EP+EF,*,F,세요,Inflect,EP,EF,시/EP/*+어요/EF/*', 'UNKNOWN,*,*,*,*,*,*,*', 'NNG,*,F,위,*,*,*,*', 'JKB,*,F,에,*,*,*,*', 'JX,*,T,는,*,*,*,*', 'MAG,성분부사|양태부사,T,달달,*,*,*,*', 'XSA+ETM,*,T,한,Inflect,XSA,ETM,하/XSA/*+ᆫ/ETM/*', 'NNG,*,F,바닐라,*,*,*,*', 'NNG,*,F,아이스,*,*,*,*', 'VCP+EC,*,F,고,Inflect,VCP,EC,이/VCP/*+고/EC/*', 'NNG,장소,T,밑,*,*,*,*', 'JKB,*,F,에,*,*,*,*', 'JX,*,T,는,*,*,*,*', 'VV+ETM,*,T,얼린,Inflect,VV,ETM,얼리/VV/*+ᆫ/ETM/*', 'NNG,*,F,딸기,*,*,*,*', 'JKB,*,F,에,*,*,*,*', 'NNG,*,F,연유,*,*,*,*', 'JKS,*,F,가,*,*,*,*', 'VV+EC+VX+EC,*,F,뿌려져,Inflect,VV,EC,뿌리/VV/*+어/EC/*+지/VX/*+어/EC/*']\n"
     ]
    }
   ],
   "source": [
    "## 출력 확인용\n",
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "real = tagger.parse(test[0])\n",
    "seperated_text = real.split()[1::2] # step = 2, 짝수번째 인덱스에는 글자만 들어있음 ex) text[0] ==> '사람', [2] ==> '들', \n",
    "print(seperated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NNG', '사람'), ('VV', '모르'), ('XR', '유명'), ('VV', '하'), ('NNP', '일본'), ('NNG', '편의점'), ('NNG', '음식'), ('NNP', '일본'), ('NNP', '미니스톱'), ('NNP', '아이스'), ('NNG', '과'), ('NNG', '세상'), ('NNG', '사람'), ('VV', '먹'), ('UNKNOWN', '*'), ('NNG', '위'), ('NNG', '바닐라'), ('NNG', '아이스'), ('NNG', '밑'), ('NNG', '딸기'), ('NNG', '연유')]\n"
     ]
    }
   ],
   "source": [
    "custom_list =  [\"NNP\",\"NNG\",\"VV\",\"VA\",\"XR\", \"UNKNOWN\"]\n",
    "\n",
    "words = []\n",
    "types = []\n",
    "\n",
    "for i in seperated_text:\n",
    "    tmp = i.split(',')\n",
    "#     print(tmp)\n",
    "    if tmp[0] in custom_list:\n",
    "        types.append(tmp[0])\n",
    "        words.append(tmp[3])\n",
    "    \n",
    "\n",
    "print(list(zip(types, words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `품사가 필요한 경우`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 문장별로 실행되는 함수 => 텍스트를 for문으로 받아서 사용\n",
    "\n",
    "def morph_extract(after_rs_text, custom_list =  [\"NNP\",\"NNG\",\"VV\",\"VA\",\"XR\", \"UNKNOWN\"]):\n",
    "    tagger = MeCab.Tagger()\n",
    "    real = tagger.parse(after_rs_text)\n",
    "    seperated_text = real.split()[1::2] # step = 2, 짝수번째 인덱스에는 글자만 들어있음 ex) text[0] ==> '사람', [2] ==> '들', \n",
    "\n",
    "    words = []\n",
    "    types = []\n",
    "\n",
    "    for i in seperated_text:\n",
    "        tmp = i.split(',')\n",
    "        \n",
    "        if tmp[0] in custom_list:\n",
    "            types.append(tmp[0])\n",
    "            words.append(tmp[3])\n",
    "\n",
    "    ans = list(zip(types, words))\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NNG', '오랫만'),\n",
       " ('NNG', '아이스'),\n",
       " ('NNG', '돌체'),\n",
       " ('NNG', '라떼'),\n",
       " ('VA', '맛있'),\n",
       " ('NNP', '부산광역시')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_extract(test[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과값 (morph_extract(test[2]))\n",
    "```python\n",
    "[('NNG', '오랫만'),\n",
    " ('NNG', '아이스'),\n",
    " ('NNG', '돌체'),\n",
    " ('NNG', '라떼'),\n",
    " ('VA', '맛있'),\n",
    " ('NNP', '부산광역시')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[[('NNG', '사람'), ('VV', '모르'), ('XR', '유명'), ('VV', '하'), ('NNP', '일본'), ('NNG', '편의점'), ('NNG', '음식'), ('NNP', '일본'), ('NNP', '미니스톱'), ('NNP', '아이스'), ('NNG', '과'), ('NNG', '세상'), ('NNG', '사람'), ('VV', '먹'), ('UNKNOWN', '*'), ('NNG', '위'), ('NNG', '바닐라'), ('NNG', '아이스'), ('NNG', '밑'), ('NNG', '딸기'), ('NNG', '연유')], [('NNG', '아이스'), ('NNG', '오락'), ('NNG', '시간'), ('NNG', '아이스'), ('NNG', '오락'), ('NNG', '시간'), ('NNG', '아이스'), ('XR', '유연'), ('NNG', '왕')], [('NNG', '오랫만'), ('NNG', '아이스'), ('NNG', '돌체'), ('NNG', '라떼'), ('VA', '맛있'), ('NNP', '부산광역시')], [('NNG', '맛집'), ('NNG', '탐험대'), ('VV', '놀라'), ('NNG', '커피'), ('NNG', '테라'), ('NNG', '날씨'), ('NNG', '냉커피'), ('VA', '덥'), ('NNG', '맛'), ('NNG', '맛집'), ('NNG', '사장'), ('NNG', '서정'), ('NNG', '점'), ('NNP', '아메리카노'), ('NNP', '아이스'), ('NNG', '아이스커피'), ('NNG', '양평'), ('VA', '좋'), ('NNG', '카페'), ('NNG', '커피'), ('NNG', '커피'), ('NNG', '향'), ('NNG', '커피'), ('NNG', '향기'), ('NNG', '향기'), ('NNG', '홍보')], [('NNP', '렉스'), ('NNG', '아이스'), ('NNP', '이아고'), ('NNP', '쥬얼리'), ('NNG', '명품'), ('NNG', '시'), ('NNG', '계금'), ('NNP', '화이트골드'), ('NNP', '화이트골드')]]\n"
     ]
    }
   ],
   "source": [
    "## 시간 매우 오래걸림. 확인용으로 5개만 사용함\n",
    "\n",
    "after_morph_list = []\n",
    "test = rs_text[0:5]\n",
    "\n",
    "for selected_text in test:\n",
    "    after_morph_list.append(morph_extract(selected_text))\n",
    "    \n",
    "print(len(after_morph_list))\n",
    "print(after_morph_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `품사가 필요 없는 경우`, 아래의 함수로 텍스트만 가져와 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extract(after_rs_text, custom_list =  [\"NNP\",\"NNG\",\"VV\",\"VA\",\"XR\", \"UNKNOWN\"]):\n",
    "    tagger = MeCab.Tagger()\n",
    "    real = tagger.parse(after_rs_text)\n",
    "    seperated_text = real.split()[1::2] # step = 2, 짝수번째 인덱스에는 글자만 들어있음 ex) text[0] ==> '사람', [2] ==> '들', \n",
    "\n",
    "    words = []\n",
    "    types = []\n",
    "\n",
    "    for i in seperated_text:\n",
    "        tmp = i.split(',')\n",
    "        \n",
    "        if tmp[0] in custom_list:\n",
    "            words.append(tmp[3])\n",
    "            \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['사람 모르 유명 하 일본 편의점 음식 일본 미니스톱 아이스 과 세상 사람 먹 * 위 바닐라 아이스 밑 딸기 연유', '아이스 오락 시간 아이스 오락 시간 아이스 유연 왕', '오랫만 아이스 돌체 라떼 맛있 부산광역시', '맛집 탐험대 놀라 커피 테라 날씨 냉커피 덥 맛 맛집 사장 서정 점 아메리카노 아이스 아이스커피 양평 좋 카페 커피 커피 향 커피 향기 향기 홍보', '렉스 아이스 이아고 쥬얼리 명품 시 계금 화이트골드 화이트골드']\n"
     ]
    }
   ],
   "source": [
    "after_word_extract = []\n",
    "\n",
    "for selected_text in test:\n",
    "    after_word_extract.append(word_extract(selected_text))\n",
    "\n",
    "print(len(after_word_extract))\n",
    "print(after_word_extract)\n",
    "\n",
    "## 중간에 보이는 *은 Unknown값으로 들어감."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### semi-result\n",
    "\n",
    "```python\n",
    "5\n",
    "['사람 모르 유명 하 일본 편의점 음식 일본 미니스톱 아이스 과 세상 사람 먹 * 위 바닐라 아이스 밑 딸기 연유', '아이스 오락 시간 아이스 오락 시간 아이스 유연 왕', '오랫만 아이스 돌체 라떼 맛있 부산광역시', '맛집 탐험대 놀라 커피 테라 날씨 냉커피 덥 맛 맛집 사장 서정 점 아메리카노 아이스 아이스커피 양평 좋 카페 커피 커피 향 커피 향기 향기 홍보', '렉스 아이스 이아고 쥬얼리 명품 시 계금 화이트골드 화이트골드']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-2) Preprocessor - 자소분리 (Decompose) with `hgtk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅇㅏㅇㅣㅅㅡ\n",
      "ㅇㅏᴥㅇㅣᴥㅅㅡᴥ\n"
     ]
    }
   ],
   "source": [
    "## 예시 코드\n",
    "\n",
    "import hgtk\n",
    "\n",
    "a = hgtk.text.decompose(\"아이스\")\n",
    "print(a.replace(\"ᴥ\",\"\"))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_text(text_list):\n",
    "    new_list = []\n",
    "    for text in text_list:\n",
    "        decomposed = hgtk.text.decompose(text)\n",
    "        new_list.append(decomposed.replace(\"ᴥ\",\"\"))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_alpha(text_list):\n",
    "    new_list = []\n",
    "    alpha = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    for text in text_list:\n",
    "        one = \"\"\n",
    "        for m in text.lower():\n",
    "            if m not in alpha :\n",
    "                one += m\n",
    "        new_list.append(one)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_text = decompose_text(test) # 자소분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra_text = remove_alpha(d_text) # 알파벳 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ㅅㅏㄹㅏㅁㄷㅡㄹㅇㅣ ㅈㅏㄹ ㅁㅗㄹㅡㄴㅡㄴㄷㅔ ㅈㅣㄴㅉㅏ ㅇㅠㅁㅕㅇㅎㅐㅈㅕㅆㅇㅡㅁㅕㄴ ㅎㅏㄴㅡㄴ ㅇㅣㄹㅂㅗㄴ ㅍㅕㄴㅇㅢㅈㅓㅁ ㅇㅡㅁㅅㅣㄱ ㅇㅣㄹㅂㅗㄴ ㅁㅣㄴㅣㅅㅡㅌㅗㅂ ㅇㅏㅇㅣㅅㅡ  ㅅㅔㅅㅏㅇㅅㅏㄹㅏㅁㄷㅡㄹ ㅇㅣㄱㅓㅅ ㅈㅗㅁ ㅁㅓㄱㅇㅓㅂㅗㅅㅔㅇㅛㅠㅇㅠㅇㅠㅠㅠㅠ ㅇㅟㅇㅔㄴㅡㄴ ㄷㅏㄹㄷㅏㄹㅎㅏㄴ ㅂㅏㄴㅣㄹㄹㅏ ㅇㅏㅇㅣㅅㅡㄱㅗ ㅁㅣㅌㅇㅔㄴㅡㄴ ㅇㅓㄹㄹㅣㄴ ㄸㅏㄹㄱㅣㅇㅔ ㅇㅕㄴㅇㅠㄱㅏ ㅃㅜㄹㅕㅈㅕ',\n",
       " 'ㅇㅏㅇㅣㅅㅡㅇㅢㅇㅗㄹㅏㄱㅅㅣㄱㅏㄴ ㅇㅏㅇㅣㅅㅡ ㅇㅢ ㅇㅗㄹㅏㄱㅅㅣㄱㅏㄴ ㄱㅘㅇㅕㄴ ㄴㅜㄱㅏ ㅇㅏㅇㅣㅅㅡㅇㅢ ㅇㅠㅇㅕㄴㅅㅓㅇ ㅇㅘㅇㅇㅣㄹㅈㅣ',\n",
       " 'ㅇㅗ ㅇㅗㄹㅐㅅㅁㅏㄴㅇㅔ ㅇㅏㅇㅣㅅㅡ ㄷㅗㄹㅊㅔㄹㅏㄸㅔ ㅇㅡㅁ ㅁㅏㅅㅇㅣㅆㄱㅔㅆㄷㅏ    ㅂㅜㅅㅏㄴㄱㅘㅇㅇㅕㄱㅅㅣ ',\n",
       " 'ㅁㅏㅅㅈㅣㅂㅌㅏㅁㅎㅓㅁㄷㅐ ㄴㅏㄹㅡㄹ ㄴㅗㄹㄹㅏㄱㅔㅎㅏㄴ ㅋㅓㅍㅣ   ㅌㅔㄹㅏㄹㅗㅅㅏ ㄴㅏㄹㅆㅣ ㄴㅐㅇㅋㅓㅍㅣ ㄷㅓㅂㄷㅏ ㅁㅏㅅ ㅁㅏㅅㅈㅣㅂ ㅅㅏㅈㅏㅇㄴㅣㅁ ㅅㅓㅈㅓㅇㅈㅓㅁ ㅇㅏㅁㅔㄹㅣㅋㅏㄴㅗ ㅇㅏㅇㅣㅅㅡ ㅇㅏㅇㅣㅅㅡㅋㅓㅍㅣ ㅇㅑㅇㅍㅕㅇ ㅈㅗㅎㄷㅏ ㅋㅏㅍㅔ ㅋㅓㅍㅣ ㅋㅓㅍㅣㅎㅑㅇ ㅋㅓㅍㅣㅎㅑㅇㄱㅣ ㅎㅑㅇㄱㅣ ㅎㅗㅇㅂㅗ',\n",
       " 'ㄹㅗㄹㅔㄱㅅㅡ ㅇㅏㅇㅣㅅㅡ ㄷㅏㅇㅣㅇㅏㄱㅗㅆㅣㅈㅠㅇㅓㄹㄹㅣㅁㅕㅇㅍㅜㅁㅅㅣㄱㅖㄱㅡㅁㅎㅘㅇㅣㅌㅡㄱㅗㄹㄷㅡㅎㅘㅇㅣㅌㅡㄱㅗㄹㄷㅡ']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ready to use (end preprocess)\n",
    "\n",
    "```python\n",
    "\n",
    "['ㅅㅏㄹㅏㅁㄷㅡㄹㅇㅣ ㅈㅏㄹ ㅁㅗㄹㅡㄴㅡㄴㄷㅔ ㅈㅣㄴㅉㅏ ㅇㅠㅁㅕㅇㅎㅐㅈㅕㅆㅇㅡㅁㅕㄴ ㅎㅏㄴㅡㄴ ㅇㅣㄹㅂㅗㄴ ㅍㅕㄴㅇㅢㅈㅓㅁ ㅇㅡㅁㅅㅣㄱ ㅇㅣㄹㅂㅗㄴ ㅁㅣㄴㅣㅅㅡㅌㅗㅂ ㅇㅏㅇㅣㅅㅡ  ㅅㅔㅅㅏㅇㅅㅏㄹㅏㅁㄷㅡㄹ ㅇㅣㄱㅓㅅ ㅈㅗㅁ ㅁㅓㄱㅇㅓㅂㅗㅅㅔㅇㅛㅠㅇㅠㅇㅠㅠㅠㅠ ㅇㅟㅇㅔㄴㅡㄴ ㄷㅏㄹㄷㅏㄹㅎㅏㄴ ㅂㅏㄴㅣㄹㄹㅏ ㅇㅏㅇㅣㅅㅡㄱㅗ ㅁㅣㅌㅇㅔㄴㅡㄴ ㅇㅓㄹㄹㅣㄴ ㄸㅏㄹㄱㅣㅇㅔ ㅇㅕㄴㅇㅠㄱㅏ ㅃㅜㄹㅕㅈㅕ',\n",
    " 'ㅇㅏㅇㅣㅅㅡㅇㅢㅇㅗㄹㅏㄱㅅㅣㄱㅏㄴ ㅇㅏㅇㅣㅅㅡ ㅇㅢ ㅇㅗㄹㅏㄱㅅㅣㄱㅏㄴ ㄱㅘㅇㅕㄴ ㄴㅜㄱㅏ ㅇㅏㅇㅣㅅㅡㅇㅢ ㅇㅠㅇㅕㄴㅅㅓㅇ ㅇㅘㅇㅇㅣㄹㅈㅣ',\n",
    " 'ㅇㅗ ㅇㅗㄹㅐㅅㅁㅏㄴㅇㅔ ㅇㅏㅇㅣㅅㅡ ㄷㅗㄹㅊㅔㄹㅏㄸㅔ ㅇㅡㅁ ㅁㅏㅅㅇㅣㅆㄱㅔㅆㄷㅏ    ㅂㅜㅅㅏㄴㄱㅘㅇㅇㅕㄱㅅㅣ ',\n",
    " 'ㅁㅏㅅㅈㅣㅂㅌㅏㅁㅎㅓㅁㄷㅐ ㄴㅏㄹㅡㄹ ㄴㅗㄹㄹㅏㄱㅔㅎㅏㄴ ㅋㅓㅍㅣ   ㅌㅔㄹㅏㄹㅗㅅㅏ ㄴㅏㄹㅆㅣ ㄴㅐㅇㅋㅓㅍㅣ ㄷㅓㅂㄷㅏ ㅁㅏㅅ ㅁㅏㅅㅈㅣㅂ ㅅㅏㅈㅏㅇㄴㅣㅁ ㅅㅓㅈㅓㅇㅈㅓㅁ ㅇㅏㅁㅔㄹㅣㅋㅏㄴㅗ ㅇㅏㅇㅣㅅㅡ ㅇㅏㅇㅣㅅㅡㅋㅓㅍㅣ ㅇㅑㅇㅍㅕㅇ ㅈㅗㅎㄷㅏ ㅋㅏㅍㅔ ㅋㅓㅍㅣ ㅋㅓㅍㅣㅎㅑㅇ ㅋㅓㅍㅣㅎㅑㅇㄱㅣ ㅎㅑㅇㄱㅣ ㅎㅗㅇㅂㅗ',\n",
    " 'ㄹㅗㄹㅔㄱㅅㅡ ㅇㅏㅇㅣㅅㅡ ㄷㅏㅇㅣㅇㅏㄱㅗㅆㅣㅈㅠㅇㅓㄹㄹㅣㅁㅕㅇㅍㅜㅁㅅㅣㄱㅖㄱㅡㅁㅎㅘㅇㅣㅌㅡㄱㅗㄹㄷㅡㅎㅘㅇㅣㅌㅡㄱㅗㄹㄷㅡ']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Word Embedding  (data transform & Tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw데이터를 훈련-테스트 데이터로 나눈후 , fitting해서 리턴\n",
    "def data_transformer(data):\n",
    "    \n",
    "    #data = pd.read_excel(xlsx_file,encoding = 'utf-8')\n",
    "    #df = pd.DataFrame({'text':data['text'], 'label':data['label']})\n",
    "    Train_X, Test_X, Train_Y, Test_Y = train_test_split(data['Text'],data['Label'],test_size=0.3, random_state = 1)\n",
    "    \n",
    "    Tfidf_vect = TfidfVectorizer(max_features = 100000)\n",
    "    Tfidf_vect.fit(Train_X[:].values.astype('U'))\n",
    "\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(Train_X[:].values.astype('U'))\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(Test_X[:].values.astype('U'))\n",
    "\n",
    "    Tfidf_vect = TfidfVectorizer(max_features = 100000)\n",
    "    Tfidf_vect.fit(Train_X[:].values.astype('U'))\n",
    "\n",
    "    Train_X_Tfidf = Tfidf_vect.transform(Train_X[:].values.astype('U'))\n",
    "    Test_X_Tfidf = Tfidf_vect.transform(Test_X[:].values.astype('U'))\n",
    "    \n",
    "    return Train_X_Tfidf, Test_X_Tfidf, Train_Y, Test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models(Train_X, Test_X, Train_Y, Test_Y):\n",
    "    def __init__(self, Train_X, Test_X, Train_Y, Test_Y) # 아직 감이 안온다. 좀 더 해보자"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
